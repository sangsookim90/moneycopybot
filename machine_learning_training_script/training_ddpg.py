# -*- coding: utf-8 -*-
"""training_ddpg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RPXAWAeVSH6U6DsmBNMIT9LJrPS2oqDV

1. input = [state, action, reward, state]
- state  [None, 6 ,11]
  : 1시간 동안의 데이터 (or 등락 데이터를 봐야됨) : 
- action : [1] 
'BUY', 'SELL', '유지' 
'얼마나 살껀가?' --- 그냥 가격으로 해서 주를 사는 걸로 하자. 최대 100만원 이하? 
- reward : 1시간 이후의 수익률 ( 더 줄이거나 늘려도 됨 )  
- 어떤 주를 살건가 ? : 이거는 고민을 좀 더 해봐야 되는 부분임.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
from google.colab import drive
import pandas as pd
import copy
import torch.nn.functional as F

drive.mount('/drive/')
dir_path = '/drive/MyDrive/lstm_file/'

file_names = os.listdir(dir_path)
file_names = [dir_path + x  for x in file_names if 'csv' in x ]

class Env(): 
  def __init__(self, file_names, state_size, batch_size): 
    self.state_size = state_size
    self.batch_size = batch_size
    self.columns = ['OPEN','HIGH','LOW','CLOSE','JDIFF_VOL','VALUE']
    self.file_names = file_names

  def simulate(self): 
    states = []
    next_states = [] 
    start_prices = [] 
    end_prices = []
    actions = (np.random.randint(0,2,self.batch_size) - 1/2 )* 2
    for i in range(self.batch_size):
      file_idx = np.random.choice(list(range(len(self.file_names))))
      file_name = self.file_names[file_idx]
      
      data = pd.read_csv(file_name, index_col = 0)
      data['TIME'] = data['TIME'].astype('str').apply(lambda x : x.zfill(6))
      data['DATETIME'] = data['DATE'].astype('str') + data['TIME'].astype('str')
      data = data.sort_values(by = 'DATETIME').reset_index(drop = True)
      idx = np.random.choice(list(range(data.shape[0]- 2*self.state_size)), 1, replace=False)
      
      action = actions[i]
      state, next_state, start_price, end_price = self.get_state_by_idx(data, idx[0], self.state_size, action)
      states.append(state)
      next_states.append(next_state)
      start_prices.append(start_price)
      end_prices.append(end_price)

    states = np.array(states).astype(float)
    next_states = np.array(next_states).astype(float)
    start_prices =np.array(start_prices).astype(float)
    end_prices = np.array(end_prices).astype(float)

    return states, next_states, start_prices/100, end_prices/100
    
  def get_state_by_idx(self,data, idx, state_size, action):
    state = np.array(data.iloc[idx:idx+state_size][self.columns]).astype(float)
    next_state = np.array(data.iloc[idx+state_size:idx+2*state_size][self.columns]).astype(float)
    state[:,0:6] = (state[:,0:6]/ (state[0,0:6]+1))
    next_state[:,0:6] = (next_state[:,0:6] / (next_state[0,0:6]+1))
    reward = self.calc_yield(data.iloc[idx+state_size]['CLOSE'], data.iloc[idx+2*state_size]['CLOSE'], action)
    return state, next_state, np.array(data.iloc[idx+state_size]['CLOSE']), np.array(data.iloc[idx+2*state_size]['CLOSE'])


  def calc_yield(self, first_price, last_price, action, n = 10): 
      return (last_price - first_price) * n * action

#### Critic Model
#### 예상 Reward 를 예측하는 것
#### Input : 현재 state, 현재 action 
#### output : 예상 수익률 (Rewards)
class CriticModel(nn.Module): 
  def __init__(self, state_col_size, action_size, hidden_size = 30) : 
    super().__init__()
    self.state_col_size = state_col_size 
    self.action_size = action_size 
    self.hidden_size = hidden_size
    self.lstm = nn.LSTM(
        input_size = state_col_size ,
        hidden_size = hidden_size, 
        num_layers = 4, 
        batch_first = True,
        bidirectional = False, 
        bias = False
    )
    self.fc1 = nn.Linear(self.hidden_size + self.action_size, 20)
    self.fc2 = nn.Linear(20, 1)
    self.fc1.weight = torch.nn.init.xavier_uniform_(self.fc1.weight)
    self.fc2.weight = torch.nn.init.xavier_uniform_(self.fc2.weight)



  def forward(self, state, action): 
    self.lstm.flatten_parameters()
    y, _ = self.lstm(state)
    y = y[:,-1,:]
    y = torch.cat([y, action], axis = 1)
    y = self.fc1(y)
    y = torch.tanh(y)
    y = self.fc2(y)
    return y 


# critic = CriticModel(9, 1, hidden_size = 50).float()
# critic.float()
# critic(torch.from_numpy(np.array(states)).float(), torch.from_numpy(np.array(actions)).float())

#### Actor Model
#### Input : 1시간 동안 데이터인데 60분 동안 가지고 있음.
#### Output : + 이면 산다. - 이면 판다. 사이에 있다 그러면 threshold 정해서 사거나 팔지 않는다. 

class ActorModel(nn.Module): 
  def __init__(self, state_col_size, hidden_size = 50) : 
    super().__init__()
    self.state_size = state_col_size 
    self.hidden_size = hidden_size
    self.lstm = nn.LSTM(
        input_size = state_col_size, 
        hidden_size = hidden_size, 
        num_layers = 4, 
        batch_first = True,
        bidirectional = False, 
        bias = False
    )
    self.fc1 = nn.Linear(self.hidden_size, 5)
    self.fc2 = nn.Linear(5, 3)

    self.fc1.weight = torch.nn.init.xavier_uniform_(self.fc1.weight)
    self.fc2.weight = torch.nn.init.xavier_uniform_(self.fc2.weight)
    self.T = 0.1
    self.eps = 0.2


  def forward(self, x): 
    self.lstm.flatten_parameters()
    y, _ = self.lstm(x)
    y = self.fc1(y)
    y = torch.tanh(y)
    y = self.fc2(y)
    y= torch.softmax(y[:,-1,:], dim = 1) / self.T
    y = torch.argmax(y, axis = 1)  
    y = torch.unsqueeze(y, 1) -1
    if self.training : 
      y = torch.where( torch.rand_like(y.float()) < self.eps , torch.randint_like(y,3)-1, y)
    self.eps *= 0.99

    return y

# actor = ActorModel(9, hidden_size = 50).float()
# actor(torch.from_numpy(np.array(states)).float())

# #### Actor Model
# #### Input : 1시간 동안 데이터인데 60분 동안 가지고 있음.
# #### Output : + 이면 산다. - 이면 판다. 사이에 있다 그러면 threshold 정해서 사거나 팔지 않는다. 

# class ActorModel(nn.Module): 
#   def __init__(self, state_col_size, hidden_size = 50) : 
#     super().__init__()
#     self.state_size = state_col_size 
#     self.hidden_size = hidden_size
#     self.lstm = nn.LSTM(
#         input_size = state_col_size, 
#         hidden_size = hidden_size, 
#         num_layers = 4, 
#         batch_first = True,
#         bidirectional = False
#     )
#     self.fc1 = nn.Linear(self.hidden_size, 5)
#     self.fc2 = nn.Linear(5,1)
#     torch.nn.init.xavier_uniform(self.lstm.parameters())
#     torch.nn.init.xavier_uniform(self.fc1.weight)
#     torch.nn.init.xavier_uniform(self.fc2.weight)

#   def forward(self, x): 
#     self.lstm.flatten_parameters()
#     y, _ = self.lstm(x)
#     y = self.fc1(y)
#     y = F.tanh(y)
#     y = self.fc2(y)
#     y= y[:,-1,:]
#     y = torch.sigmoid(y) + eps1
#     y = torch.where(y < 0.35, torch.zeros_like(y) -1 , y)
#     y = torch.where(y>0.7, 1 , 0)
#     # y = torch.unsqueeze(y, 1) -1
#     return y

# # actor = ActorModel(9, hidden_size = 50).float()
# # actor(torch.from_numpy(np.array(states)).float())

### Replay Buffer : 데이터 Correlation 성분제거 및 한번 저장한 데이터 여러번 사용하기

class Memory(): 
  def __init__(self, max_size) : 
    self.memory = []

  def append(self, data): 
    memory.append(data)
  
  def sampling(self, batch_size): 
    idxs = np.random.choice(len(memory), batch_size)
    return memory[idxs]

#torch.nn.init.xavier_uniform(conv1.weight)

class DDPG() : 
  def __init__(self, state_col_size, state_step_size,  hidden_size, action_size, batch_size, gamma, file_names, is_gpu = False, load_model = False): 
    self.state_col_size = state_col_size
    self.hidden_size = hidden_size
    self.action_size = action_size
    self.batch_size = batch_size
    self.gamma = gamma
    self.state_step_size = state_step_size
    self.is_gpu = is_gpu
    self.critic = CriticModel(self.state_col_size, self.action_size, hidden_size = self.hidden_size)
    self.critic_target = CriticModel(self.state_col_size, self.action_size, hidden_size = self.hidden_size)
    self.actor = ActorModel(self.state_col_size, hidden_size = self.hidden_size)
    self.actor_target = ActorModel(self.state_col_size, hidden_size = self.hidden_size)
    self.env = Env(file_names, self.state_step_size, self.batch_size)


    for param, param_target in zip(self.critic.parameters(), self.critic_target.parameters()):

        param_target.data.copy_(param.data)  # initialize
        param_target.requires_grad = False  # not update by gradient
        

    for param, param_target in zip(self.actor.parameters(), self.actor_target.parameters()):
        param_target.data.copy_(param.data)  # initialize
        param_target.requires_grad = False  
        

    self.critic = self.critic.float()
    self.critic_target = self.critic_target.float()
    self.actor = self.actor.float()
    self.actor_target = self.actor_target.float()


    self.critic_opt = optim.Adam(self.critic.parameters(), lr=0.001)
    self.actor_opt = optim.Adam(self.actor.parameters(), lr=0.001)
    self.actor_m = 0.001
    self.critic_m = 0.001

    if load_model : 
      if self.is_gpu : 
        checkpoint = torch.load('{0}ddpg_20210117_model_반도체_no_soft.torch'.format(dir_path))      
      else : 
        checkpoint = torch.load('{0}ddpg_20210117_model_반도체_no_soft.torch'.format(dir_path), map_location=torch.device('cpu') )          

      self.critic.load_state_dict(checkpoint['critic'])
      self.critic_target.load_state_dict(checkpoint['critic_target'])
      self.actor.load_state_dict(checkpoint['actor'])
      self.actor_target.load_state_dict(checkpoint['actor_target'])


    if self.is_gpu : 
      self.critic = self.critic.cuda()
      self.critic_target = self.critic_target.cuda()
      self.actor = self.actor.cuda()
      self.actor_target = self.actor_target.cuda()



  def calc_yield(self, start_price, end_price, actions): 
    # print('end_price : {0}'.format(end_price.size()))
    # print('start_price : {0}'.format(start_price.size()))
    # print('actions : {0}'.format(actions.size()))
    rewards = (end_price - start_price) * actions[:,0] * 10
    # print('rewards : {0}'.format(rewards.size()))
    rewards = torch.where(rewards > 0 , rewards, 10*rewards)
    return rewards

        
  def train(self, epochs = 50000): 

    self.critic.train()
    self.actor.train() 
    self.critic_target.train()
    self.actor_target.train() 

    ### save_queue 
    # memory.append( states, actions, rewards, next_states )
    # states, actions, rewards, next_states = memory.sampling(batch_size)

    for epoch in range(epochs) : 
      states, next_states, start_price, end_price = self.env.simulate()
      states  = torch.from_numpy(states)
      next_states = torch.from_numpy(next_states)
      start_price = torch.from_numpy(start_price)
      end_price  = torch.from_numpy(end_price)
      
      states = states.float()
      next_states = next_states.float()
      start_price = start_price.float()
      end_price = end_price.float()

      if self.is_gpu : 
        states = states.cuda()
        next_states = next_states.cuda()
        start_price = start_price.cuda()
        end_price = end_price.cuda()
      
      
      actions = self.actor(states)

      
      target_actions = self.actor_target(next_states)
      q_value = self.critic(states, actions)
      target_q_value = self.critic_target(next_states, target_actions)
      rewards = self.calc_yield(start_price, end_price, actions)

      ### Critic Phase 
      q_target = rewards + self.gamma * target_q_value
      td_errors = q_target.view(batch_size, -1) - q_value.view(batch_size, -1)
      critic_loss = torch.mean(td_errors ** 2)
      cl = critic_loss.item()
      self.critic_opt.zero_grad()
      critic_loss.backward()
      self.critic_opt.step()

      ### Actor Phase
      q_value = self.critic(states, actions)
      actor_loss = -torch.mean(q_value) 
      al = actor_loss.item()
      self.actor_opt.zero_grad()
      actor_loss.backward()
      self.actor_opt.step()
      ### target_param_update
      self.momentum_update()
      if epoch % 10 == 0 : 
        print(np.sum(actions.detach().cpu().numpy() == 1 ))
        print(np.sum(actions.detach().cpu().numpy() == 0 ))
        print(np.sum(actions.detach().cpu().numpy() == -1 ))
        torch.save({
                      'critic': self.critic.state_dict(),
                      'critic_target': self.critic_target.state_dict(),
                      'actor': self.actor.state_dict(),
                      'actor_target': self.actor_target.state_dict()
                      }, '{0}ddpg_20210117_model_반도체_no_soft.torch'.format(dir_path))
        print('{0} epochs, critic : {1}, actor : {2}'.format(epoch, cl, al))
    
    
  @torch.no_grad()
  def momentum_update(self) : 
    for param, param_target in zip(self.critic.parameters(), self.critic_target.parameters()):
        param_target.data = param_target.data * self.critic_m + param.data * (1. - self.critic_m)

    for param, param_target in zip(self.actor.parameters(), self.actor_target.parameters()):
        param_target.data = param_target.data * self.actor_m + param.data * (1. - self.actor_m)



  def get_model(self): 
    return self.critic, self.actor

state_col_size = 6
state_step_size = 24
hidden_size = 100
action_size = 1
batch_size = 512
gamma = 0.99
ddpg = DDPG(state_col_size, state_step_size,  hidden_size, action_size, batch_size, gamma, file_names, True, load_model =True)
ddpg.train()

env = Env(file_names, 6, 32)
states, next_states, start_price, end_price = env.simulate()
actor = ActorModel(6,32)
actor.train()
actor(torch.from_numpy(states).float())